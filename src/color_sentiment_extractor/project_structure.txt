1. Chatbot/
├── 1.1 Data/
│   ├── expression_context_rules.json
│   ├── expression_definition.json
│   ├── known_modifiers.json
│   └── products.json
│
│
├── 1.2 extraction/
│   ├── Brand/
│   ├── Color/
│       ├── extraction/
│           ├── __init__.py
│           ├── compound.py
│               ├── attempt_mod_tone_pair() - Takes a candidate modifier and tone, resolves each to a trusted form, and, if valid, combines them into a color compound. It first checks for semantic conflicts (modifier only), then tries to resolve each token against known modifiers, tones, or web color names without fuzzy matching. If unresolved, it uses an LLM-based simplification fallback and rechecks conflicts. It rejects results where modifier rewrites are untrusted or tones with "y"/"ish" suffixes are not in known tone lists. When both tokens pass all checks, it concatenates them into a "modifier tone" string and stores the compound in both a set and a list.
│               ├── is_plausible_modifier() - Check if a token is a plausible color modifier by validating against known modifiers or tones, ensuring it is alphabetic and not too short, with a soft fallback to accept unknown but valid words.
│               ├── extract_from_split() - scans each token for glued color compounds and tries to recover well‑formed 2‑ or 3‑part phrases, writing validated results into compounds (set) and raw_compounds (list). It skips tokens that are already known standalone modifiers/tones or that duplicate an existing compound (ignoring spaces), then attempts a structured split via split_tokens_to_parts against the union of known modifiers and tones. If splitting fails or yields an invalid shape, it falls back to brute force left/right cuts, normalizing the modifier side with match_suffix_fallback and recover_base, and only accepts the pair when the tone is known and the modifier is plausible (is_plausible_modifier). For successful structured splits, it checks role validity using resolve_modifier_token (for modifiers) and is_known_tone (for tones): in 2‑part cases it accepts mod‑tone or tone‑mod arrangements that resolve cleanly; in 3‑part cases it allows specific role patterns (mod‑tone‑mod, mod‑mod‑tone, mod‑tone‑tone). If a 3‑part shape is invalid but the last two tokens form a known merged tone, it accepts a mod + merged‑tone compound. Throughout, it applies suffix normalization to parts, avoids duplicates, respects known_color_tokens, and emits detailed traces when debug is enabled; the function returns nothing and operates via side effects on the provided collections.
│               ├── extract_from_glued() - scans tokens that look like single, alphabetic “glued” color strings and attempts to reconstruct valid 2‑ or 3‑part color phrases, writing accepted results into compounds (set) and raw_compounds (list). It immediately skips tokens that are already known color tokens or contain non‑alphabetic characters, then tries a dedicated glued‑token splitter; if that fails or yields an invalid shape, it falls back to a generic splitter to probe a 3‑part structure and, when the first part can be normalized to a known modifier and the remaining parts resolve as known tones, it records the phrase. For successful structured splits: in 2‑part cases it prefers a resolved modifier for the first part via resolve_modifier_token and requires the second to be a known tone (with allowances for edge role pairings when both sides are recognized), while in 3‑part cases it evaluates role patterns (modifier/tone combinations) by resolving each part both as potential modifiers and tones (resolve_modifier_token and is_known_tone) and accepts only if the configuration matches allowed patterns. Throughout, it avoids duplicates, applies minimal suffix‑style normalization only in the fallback 3‑part path, emits detailed debug traces when debug is enabled, and returns None, operating purely through side effects on the provided collections.
│               ├── extract_from_adjacent() - iterates across a token sequence and builds color compounds from locally adjacent tokens by testing two structures: (1) a single candidate modifier immediately followed by a tone, and (2) a two‑word modifier formed by hyphenating the current and next tokens, followed by a tone. For each position, it lowercases inputs, singularizes the tone candidate(s), resolves modifier candidates with resolve_modifier_token (disallowing tone resolution in that call), and verifies tones strictly against known_tones. When a structure resolves cleanly, it constructs the "modifier tone" phrase, deduplicates against compounds, and records accepted results into both compounds (set) and raw_compounds (list). The function performs bounds checks for the two‑word path, emits detailed trace messages when debug is enabled, mutates the provided collections in place, and returns None.
│               ├── extract_compound_phrases() - orchestrates end‑to‑end extraction of color compounds from a token sequence (or from raw_text, which it pre‑normalizes by replacing hyphens with spaces and re‑tokenizes) and records results via side effects in compounds (set of strings) and raw_compounds (list of strings or (mod, tone) tuples). It runs three primary extractors in order—extract_from_adjacent, extract_from_split, and extract_from_glued—to capture modifier–tone phrases using adjacency, structured splitting, and glued‑token recovery, leveraging the supplied vocabularies (known_color_tokens, known_modifiers, known_tones, all_webcolor_names). After the main passes, it applies a fuzzy‑enabled fallback over adjacent pairs: it normalizes the left token, skips if it is already a known modifier/tone or too short, requires the right token to be a known tone and alphabetic, then calls resolve_modifier_token(..., allow_fuzzy=True) to recover a plausible modifier and, if successful and new, adds both "mod tone" to compounds and (mod, tone) to raw_compounds. Finally, it enforces semantic safety by building a set of blocked (modifier, tone) pairs via is_blocked_modifier_tone_pair, removing any offending phrases from compounds and their tuple entries from raw_compounds; optional debug tracing logs each stage, and the function returns None.
│           ├── standalone.py
│               ├── extract_lone_tones() - scans the input token stream and returns a set of standalone color tones that are present verbatim in known_tones, with all comparisons done on a normalized form (normalize_token). For each token, it first filters out cosmetic nouns listed in COSMETIC_NOUNS, then accepts only tokens whose normalized value is a member of known_tones, adding them to a matches set (naturally deduplicating repeats). It performs no fuzzy matching or LLM fallback, does not attempt compound reconstruction, and limits output strictly to single‑token tones; optional debug logging reports each cosmetic block and accepted tone before the final set is returned.
│               ├── _finalize_standalone_phrases() - merges two collections of standalone color terms—injected (expression-injected modifiers) and filtered (resolved token matches)—into a single unified set by converting each to a set (handling None as empty) and performing a set union. It optionally logs the final combined set when debug is enabled, and returns this merged set as the output.
│               ├── extract_sandalone_phrases() - identifies and returns a set of valid standalone color modifiers and tones from a token sequence by combining three complementary strategies. First, it calls _inject_expression_modifiers() to add modifiers derived from expression-based mapping rules (expression_map). Second, it applies _extract_filtered_tokens() to the same tokens, using rule-based checks and an LLM fallback to resolve potential modifiers and tones with higher flexibility. Third, it merges the results from both steps with _finalize_standalone_phrases(), producing a unified, deduplicated set of terms. Throughout, it can emit detailed debug logs showing token details, intermediate matches from each stage, and the final combined output, and it returns the merged set as the result.
│       ├── llm/
│           ├── __init__.py
│           ├── llm_api_clients.py
│               ├── build_color_prompt() - creates and returns a formatted natural language string instructing an LLM to produce the RGB tuple for a given descriptive color phrase. The prompt clearly specifies the phrase to match, enforces a strict (R, G, B) output format with no extra explanation, and includes a few example mappings of phrases to RGB values before ending with the target phrase in the same pattern.
│               ├── build_llm_request_payload() - prepares and returns a dictionary formatted for an OpenAI-style LLM API request to resolve an RGB value for a given color phrase. It first generates the query text via build_color_prompt(color_phrase), then wraps it with the configured model name (LLM_MODEL), token limit (LLM_MAX_TOKENS), temperature (LLM_TEMPERATURE), and a messages list containing a single user role entry with the constructed prompt.
│               ├── build_llm_headers() - creates and returns a dictionary of HTTP headers for an LLM API request, inserting the provided API key into the Authorization field as a Bearer token and setting Content-Type to application/json to indicate a JSON request body.
│               ├── query_llm_for_rgb() - retrieves an RGB color tuple for a descriptive color phrase by querying an LLM API, with optional caching, retries, and debug logging. It first checks for the OPENROUTER_API_KEY in the environment and aborts if missing. If a cache object is provided, it attempts to return a cached RGB result for the phrase. Otherwise, it builds the request payload (build_llm_request_payload) and headers (build_llm_headers), then makes up to retries + 1 POST requests to LLM_API_URL. On each attempt, it logs the query if debug is enabled, skips to the next retry on non-200 status codes, and parses the first choice’s message content using _parse_rgb_tuple. If parsing succeeds, it stores the result in the cache (if available) and returns the (R, G, B) tuple. It logs and waits progressively longer between retries on HTTP failures or exceptions, and returns None if all attempts fail.
│       ├── logic/
│           ├── __init__.py
│           ├── color_categorizer.py
│               ├── build_tone_modifier_mappings() - processes a list of two-word descriptive color phrases to identify valid modifier–tone relationships and construct bidirectional mappings. It normalizes each token with normalize_token and only processes phrases split cleanly into two parts. The first part (modifier candidate) is validated against known_modifiers directly or via a recovered base form from recover_base to handle suffix variants (e.g., "glowy" → "glow"). The second part (tone candidate) must exist in known_tones. For each valid pair, it adds the tone to a global tones set, the modifier to a global modifiers set, links the modifier to the tone in mod_to_tone, and links the tone to the modifier in tone_to_mod. It returns a tuple containing all matched tones, all matched modifiers, the modifier-to-tones mapping, and the tone-to-modifiers mapping.
│               ├── format_tone_modifier_mappings() - generates a sorted, user-friendly dictionary representation of relationships between modifiers and tones extracted from a list of descriptive color phrases. It first calls build_tone_modifier_mappings() to obtain sets of tones, modifiers, and the bidirectional mappings. It then formats the results into a dictionary with two top-level keys: "modifiers", mapping each modifier to a sorted list of its associated tones, and "tones", mapping each tone to a sorted list of its associated modifiers. This structure is intended for display, export, or other human-readable outputs.
│           ├── color_pipeline.py
│               ├── extract_all_descriptive_color_phrases() - runs a complete NLP pipeline to identify all valid descriptive color phrases in raw text, combining compound phrase, standalone term, and lone tone extraction. It tokenizes the input using spacy’s English model, then calls extract_compound_phrases() to populate a set of full modifier–tone phrases and a list of raw (modifier, tone) tuples. It augments the set with results from extract_standalone_phrases() (expression-injected and rule/LLM-resolved single terms) and extract_lone_tones() (isolated tone words), ensuring all phrases are lowercased and deduplicated. The function returns a list of these extracted phrases, representing all recognized descriptive color expressions in the input.
│               ├── extract_phrases_from_segment() - identifies and validates color-related phrases within a single text segment, returning only high-confidence, non-conflicting matches. It begins by running extract_all_descriptive_color_phrases() to gather all potential phrases, then evaluates each for role validity and match strength. Single-token phrases are accepted only if they exactly match a known tone after recover_base() normalization, pass a fuzzy confidence check (is_high_confidence), and are not in BLOCKED_TOKENS. Two-token phrases are accepted if their recovered bases form valid modifier/tone role combinations (including tone–tone), with both tokens meeting the confidence threshold. After this, it applies filters to remove phrases ending with cosmetic nouns, to discard standalone tokens already used in longer compounds, and (optionally) to restrict to phrases appearing verbatim in the segment. Debug mode logs each decision, and the function returns the final filtered set of validated color phrases.
│               ├── process_segment_colors() - takes a list of raw color phrases and runs each through the standard simplification and RGB‑resolution pipeline by calling process_color_phrase for every item, passing through vocab sets and optional llm_client/cache plus debug. For each phrase it collects the simplified textual form and the resolved RGB (or None on failure), preserving order, and finally returns a tuple containing the list of simplified phrases and the parallel list of RGB tuples (or None entries).
│               ├── aggregate_color_phrase_results() - iterates over raw text segments to extract, validate, simplify, and color‑resolve descriptive color phrases, returning a tuple of (set of simplified tone names, list of all simplified phrases in encounter order, and a phrase→RGB map). For each segment it builds an “allowed” token set from raw tokens, hyphen/space variants, and recover_base outputs; runs extract_phrases_from_segment to get candidate phrases; filters candidates with phrase_ok (all words must be allowed); then passes them through process_segment_colors to obtain simplified forms and RGBs (optionally using llm_client and cache). It preserves known tones when simplification would change them, nulls RGB when a non‑tone loses its modifier during simplification, and only accepts simplified phrases still consistent with the allowed token set. Throughout it uses KNOWN_TONES, known_modifiers, all_webcolor_names, and an expression_map loaded at runtime, with optional debug logging.
│           ├── rgb_pipeline.py
│               ├── get_rgb_from_descriptive_color_llm_first() - resolves an RGB tuple for a descriptive color string using a three-stage strategy. First, it calls query_llm_for_rgb() to attempt a direct LLM lookup, returning immediately on success. If that fails, it uses simplify_color_description_with_llm() to normalize the phrase, then tries _try_simplified_match() to find an exact match in known CSS/XKCD color definitions. If still unresolved, it performs a fuzzy match with fuzzy_match_rgb_from_known_colors(). At each step it logs progress when debug is enabled, and returns the first successful RGB match found or None if all stages fail.
│               ├── resolve_rgb_with_llm() - is a simple entry point for obtaining an RGB tuple from a descriptive color phrase. It delegates entirely to get_rgb_from_descriptive_color_llm_first(), passing through the phrase, LLM client, optional cache, and debug flag, and returns either the resolved (R, G, B) tuple or None if no match is found.
│               ├── process_color_phrase() - runs a complete normalization and RGB-resolution pipeline for a descriptive color phrase. It first applies simplify_phrase_if_needed() to reduce the phrase using rules and suffix handling; if no change occurs and an LLM client is available, it calls simplify_color_description_with_llm() as a fallback. It then scans the simplified phrase for tokens involved in SEMANTIC_CONFLICTS and replaces any conflicting term with its allowed counterpart. After this semantic cleanup, it calls resolve_rgb_with_llm() to obtain the RGB tuple for the simplified phrase. With debug enabled, it logs each stage’s input, transformations, and results. The function returns a tuple containing the final simplified phrase and the resolved RGB tuple, or None if no color match is found.
│       ├── recovery/
│           ├── __init__.py
│           ├── fuzzy_recovery.py
│               ├── is_suffix_root_match() - determines whether two strings (alias and token) are valid suffix/root variants of the same known base term. It first derives their base forms via recover_base() (with caching) and logs them if debug is enabled. It rejects the match if neither input changes after recovery, ensuring a transformation actually occurred. If both bases are identical, the base is in known_modifiers or known_tones, and the alias/token pair is not in SEMANTIC_CONFLICTS and does not trigger rhyming_conflict(), it accepts the match and returns True; otherwise, it returns False.
│           ├── llm_recovery.py
│               ├── _attempt_simplify_token() - tries to turn a noisy or unrecognized token into a valid known tone or modifier, using LLM simplification as the primary method and base recovery as a fallback. It first calls simplify_phrase_if_needed() with the provided vocab sets and LLM client, logging each step if debug is enabled. If simplification returns a value, it extracts either the first or last word depending on the role (modifier or tone), normalizes it, and checks membership in known_modifiers or known_tones. If the result is valid, it returns it; if not, it attempts recover_base() with fuzzy fallback to find a matching known term. If no simplification is produced or all checks fail, it returns None.
│               ├── _extract_filtered_tokens() -  scans a token stream to identify valid standalone modifier or tone candidates, applying layered resolution logic with multiple safety filters. For each token, it normalizes the text, skips any cosmetic nouns (COSMETIC_NOUNS) or connector words (POS tag "CCONJ"), then attempts rule-based resolution via resolve_modifier_token(). If that fails, it tries LLM-based simplification (simplify_phrase_if_needed()), extracting the first word of the result if it belongs to known_modifiers or known_tones. The function applies several rejection rules to prevent unsafe fuzzy matches—blocking results that are too short, compound mismatches, multi-word expansions from single tokens, or excessive fuzzy matches when strong results already exist. Valid resolved tokens are added to a result set, with detailed logging available when debug is True. It returns the final set of resolved modifier/tone candidates.
│               ├── build_prompt() - creates and returns a concise, single-line natural language question for an LLM, asking it to identify the simplified base color or tone corresponding to a given descriptive phrase.
│               ├── simplify_color_description_with_llm() - sends a descriptive color phrase to an LLM for simplification into a normalized tone or modifier form, using prompt engineering and optional caching to avoid redundant queries. It builds the query text with build_prompt(), logs it if debug is enabled, and checks the cache via cache.get_simplified() before making an LLM call. If no cached result exists, it calls llm_client.simplify() to get the simplification, stores it in the cache if provided, and logs the final response when debug is True. It returns the simplified string.
│               ├── simplify_phrase_if_needed() -conditionally simplifies a descriptive color phrase using an LLM. If an LLM client is not provided, it returns None. It logs progress when debug is enabled, and first checks whether the normalized phrase already exists in known_tones, in which case it returns the original without modification. Otherwise, it calls simplify_color_description_with_llm() (with optional caching) to attempt simplification. If the result is non-empty and different from the original, it returns the simplified form; if no change occurs, it returns the original phrase.
│           ├── modifier_resolution.py
│               ├── is_known_tone() - determines if a given word represents a recognized color tone by normalizing it with normalize_token() and checking for membership in either the known_tones set or the all_webcolor_names set. It returns True if found in either set, otherwise False.
│               ├── is_valid_tone() - Checks whether a phrase is a valid tone by normalizing it and first testing direct membership in known_tones; if not found, delegates to recover_base() (with fuzzy allowed) and returns True only if the recovered base is in known_tones.
│               ├── match_direct_modifier() -attempts to resolve a token into a known modifier using a sequence of normalization and fallback strategies. It lowercases and space-normalizes the token, first checking for a direct match in known_modifiers. If none is found, it calls recover_base() to handle suffix removal; if the recovered base is in known_modifiers, it returns it, and includes special handling for "ier" endings that map through RECOVER_BASE_OVERRIDES (e.g., "rosier" → "rosy" → "rose"). If still unmatched, it singularizes the token and checks again, and finally, for multi-word inputs, it returns the first part found in known_modifiers. It logs each decision when debug is True and returns the matched modifier or None if no valid match is found.
│               ├── match_suffix_fallback() -Attempts to resolve a noisy or suffixed token by normalizing, checking a space-collapsed form against known_modifiers/known_tones, and otherwise delegating to recover_base() (with fuzzy allowed). Returns the recovered base if it belongs to either set, else None.
│               ├── recover_y_with_fallback() - Resolves “-y/-ey” style variants by normalizing the token and delegating to recover_base() (with fuzzy allowed); returns the base if it’s in known_modifiers or known_tones, else None. (Remove any reference to calling recover_y() directly—your function now routes only through recover_base.)
│               ├── resolve_modifier_token() -Resolves a raw token to a valid modifier using a streamlined strategy: it first normalizes the token and, if known_tones is provided, immediately returns it when it’s a valid tone; next, it checks for a direct match in known_modifiers; failing that, it delegates to recover_base() (with optional fuzzy) to handle suffix variants and centralized overrides; if recover_base() yields a base that belongs to known_modifiers, that base is returned; throughout, optional debug logs trace each decision, and the function returns the first valid match found or None.
│               ├── should_suppress_compound() - checks whether a modifier–tone pair is semantically redundant and should be ignored. It returns True if the modifier and tone are identical, if the tone starts with the modifier, or if the modifier starts with the tone; otherwise, it returns False.
│               ├── is_blocked_modifier_tone_pair() -determines whether a given modifier–tone pair is explicitly prohibited based on a predefined blocklist (BLOCKED_TOKENS). It normalizes both terms with normalize_token(), checks the pair in both modifier–tone and tone–modifier order, and returns True if either appears in the blocklist; otherwise, it returns False.
│               ├── is_modifier_compound_conflict() - checks whether a given expression semantically overlaps with known modifiers. It resolves the expression via resolve_modifier_token() (with fuzzy matching enabled and known_tones forced to an empty set), then returns True if the resolved form exists in the provided modifier_tokens set; otherwise, it returns False.
│               ├── resolve_fallback_tokens() - attempts to recover tone or modifier tokens that may have been missed during earlier extraction. It normalizes each token’s text, directly adds it to the result set if it’s in known_tones, and otherwise tries to resolve it via resolve_modifier_token() using the provided modifier and tone sets. Successfully resolved modifiers are added to the set, with optional debug logging. It returns the final set of recovered modifier and tone tokens.
│       ├── suffix/
│           ├── __init__.py
│           ├── rules.py
│               ├── is_y_suffix_allowed() - decides whether a base token is eligible to take the “-y” suffix based on predefined rules. It immediately allows tokens listed in Y_SUFFIX_ALLOWLIST. It rejects bases shorter than three characters, those already ending in “y” or “e,” and those ending with a vowel. It allows bases ending in certain consonant or soft clusters (e.g., "sh", "ch", "m", "n", "t", etc.) that are likely to form valid “-y” adjectives. If none of these conditions match, it returns False.
│               ├── is_cvc_ending() - checks whether a base string ends in a consonant–vowel–consonant (CVC) pattern that permits consonant doubling when adding suffixes (e.g., "blur" → "blurry"). It requires the word to be at least three characters long, ensures the last three letters follow the CVC structure, and blocks cases where the final consonant is "w", "x", or "y". It returns True if the ending matches the allowed CVC pattern, otherwise False.
│               ├── build_y_variant() - generates a valid “-y” suffixed form of a base token if permitted by override mappings or suffix rules. It first checks Y_SUFFIX_OVERRIDE_FORMS for a predefined form (e.g., "rose" → "rosy"), then appends "y" if the base is in Y_SUFFIX_ALLOWLIST, or if is_y_suffix_allowed() returns True based on rule-based eligibility. If none of these conditions are met, it returns None. When debug is enabled, it logs whether the form was generated via override, allowlist, rules, or blocked entirely.
│               ├── build_ey_variant() - generates a valid “-ey” suffixed form of a base token when allowed by either explicit listing or rule-based conditions. It first checks if the raw or base form is in Y_SUFFIX_ALLOWLIST and, if so, appends "ey". Otherwise, it allows the form if the base is longer than two characters, does not end in "e" or "y", and ends in a consonant (non-vowel). If neither condition is met, it returns None. When debug is True, it logs whether the form was produced via allowlist, rule-based logic, or blocked.
│               ├── _default_suffix_strip() - removes a trailing “-y” from a token if present, returning the base form; if the token does not end with “y”, it returns the original token unchanged.
│               ├── _apply_reverse_override() - adjusts a recovered base token by applying reverse mappings from RECOVER_BASE_OVERRIDES. For each override token in the mapping, it strips a trailing "y" or "ed" if present (or leaves it unchanged otherwise) to compare against the given base. If a match is found, it returns the mapped override_base as the corrected form, logging the change if debug is enabled. If no match is found, it returns the original base unchanged.
│               ├── _collapse_repeated_consonant() - checks if a base token ends with a doubled final consonant (e.g., "blurr"), and if so, removes one occurrence. If the collapsed form exists in either known_modifiers or known_tones, it returns the collapsed version (logging when debug is enabled); otherwise, it returns the original base unchanged.
│       ├── token/
│           ├── __init__.py
│           ├── split.py
│               ├── split_glued_tokens() -tries to break a glued-together color token (e.g., "earthyrose") into meaningful known parts using a recursive, suffix-aware strategy. It normalizes the token, builds or uses a provided vocab (augmented with suffix variants), and defines cached helpers to check token validity (is_valid_cached) and recursively split tokens (recursive_split_cached). It first checks if the whole token is valid, then tries early direct splits where both halves are valid, and finally attempts recursive splits to find the longest valid part sequence. If no recursive match is found, it falls back to a longest-substring split via fallback_split_on_longest_substring(). It returns the list of split parts if they include any known modifiers or tokens, otherwise returns an empty list. When debug is enabled, it can log internal checks, and it always logs timing for the fallback path.
│               ├── split_tokens_to_parts() - tries to split a glued token (e.g., "creamyivory") into valid (modifier, tone) components by testing every possible split point, scoring each candidate, and returning the best match. It first checks for hyphenated forms where both sides are known tokens, then iterates through possible splits, validating each side against known_tokens or recovering base forms (via _recover_base_cached for the left side, recover_base for the right), cleaning digits when necessary, and rejecting weak recoveries for short tokens. Each direct match scores higher than a recovered one, and pairs found in BLOCKED_TOKENS are skipped. The function selects the highest-scoring valid split, preferring those with more recognized parts on ties, logs details when debug is True, and returns the best split as a list or None if no valid split is found.
│       ├── utils/
│           ├── __init__.py
│           ├── rgb_distance.py
│               ├── rgb_distance() - calculates the Euclidean distance between two RGB color tuples by summing the squared differences of their corresponding components, taking the square root of the result, and returning the value as a float representing their perceptual difference.
│               ├── is_within_rgb_margin() - checks whether two RGB color tuples are perceptually similar by calculating their Euclidean distance with rgb_distance() and returning True if the distance is less than or equal to the specified margin, otherwise False.
│               ├── choose_representative_rgb() -selects the most representative RGB value from a dictionary of color mappings by finding the tuple with the smallest total Euclidean distance to all others in the set, returning it as the central color, or None if the input is empty.
│               ├── find_similar_color_names() - takes a target RGB value and a mapping of known color names to RGB tuples, then returns a sorted list of all names whose colors fall within a given distance threshold, using is_within_rgb_margin() to determine closeness.
│               ├── fuzzy_match_rgb_from_known_colors() - fuzzy_match_rgb_from_known_colors() takes a color phrase and uses difflib.get_close_matches() to find the closest matching name from all_webcolor_names with at least 75% similarity, returning the best match if one exists or None otherwise.
│               ├── _try_simplified_match() normalizes a color name, replaces hyphens with spaces, and tries to find an exact match in the CSS4 or XKCD color maps; if found, it converts the hex code to an RGB tuple and returns it, otherwise it returns None.
│               ├── _parse_rgb_tuple() - searches a string for an RGB pattern using regex, validates that each component is between 0 and 255, and returns a (R, G, B) tuple if valid; otherwise, it logs a warning (if debug is enabled) and returns None.
│       ├──constants.py
│       ├──vocab.py
│   ├── General/
│           ├── __init__.py
│       ├── expression/
│           ├── __init__.py
│           ├── expression_helpers.py - Helpers to extract trigger vocabularies and expression-related token sets.
│               ├── get_all_trigger_tokens() - builds and returns a dictionary mapping each expression to a list of unique trigger tokens by combining its modifiers and aliases from _EXPRESSION_MAP_RAW.
│               ├── get_all_alias_tokens() - returns a set of all normalized tokens by collecting and normalizing every alias and modifier from the given expression_map.
│               ├── extract_exact_alias_tokens() - scans the text for exact matches of aliases or modifiers from expression_map, prioritizing longer multi-word forms, and returns a sorted list of normalized matches without shorter forms that are contained within longer matches.
│               ├── get_matching_expression_tags_cached() - is a cached wrapper that calls _get_matching_expression_tags using a preloaded expression map and returns the set of matching raw expression names.
│               ├── _get_normalized_alias_map() - builds and returns a dictionary mapping each normalized alias from the cached expression definitions to a list of expressions that use it.
│               ├── get_matching_expression_tags() - matches a text against aliases in the provided expression map, then resolves matched aliases back to their corresponding raw expression names using both direct expression matches and alias mappings, returning a set of matched expression names and optionally printing debug info.
│               ├── map_expression_to_tones() - takes a text, normalizes and tokenizes it, matches expressions (with caching), applies context promotion and suppression rules, then builds a dictionary mapping each matched expression to its valid tone modifiers from the cached expression definitions, printing timing diagnostics if debugging is enabled.
│               ├── _apply_suffix_ties() - takes a set of candidate modifiers and removes any root form when both it and its -y suffixed variant are present, ensuring only the -y form remains in such ties.
│               ├── _apply_semantic_conflicts_local() - resolves semantic conflicts in a set of candidate modifiers using the SEMANTIC_CONFLICTS config, which can be either a dict mapping canonical terms to variants or an iterable of conflict groups (pairs/sets). It normalizes all terms, groups candidates by conflict set, and for each group keeps a single deterministic winner—preferring the canonical form if present, otherwise the shortest normalized term, then alphabetical order. If no conflicts apply or an error occurs, it returns the original candidates unchanged.
│               ├── _inject_expression_modifiers() -builds a sorted list of normalized modifiers from a token sequence by combining direct alias and expression matches, base recovery, and fuzzy fallback. It first collects expressions matched from the raw text via _get_matching_expression_tags(), then iterates over tokens to add expressions linked to known aliases or recovered bases (preferring strict recovery, falling back to fuzzy recovery for known modifiers). From these matched expressions and any tokens that are themselves modifiers, it gathers candidate modifiers, then prunes them using _apply_suffix_ties() and _apply_semantic_conflicts_local() to resolve ties and conflicts before returning the final sorted set.
│               ├── apply_expression_context_rules() - scans unmatched expressions against a context rule map and promotes those whose rules are satisfied by the token set. For each unmatched expression, it checks whether at least one required token and one context clue from any of its rules appear in the input tokens; if so, it adds the expression to the promotions set and stops checking further rules for it. The function returns the set of all such promoted expression names.
│               ├── apply_expression_suppresion_rules() - filters a set of matched expressions by removing any expressions that appear in the suppression list of a dominant expression present in the set, ensuring that lower-priority expressions are excluded whenever a higher-priority one is matched, and returns the pruned set.
│               ├── get_glued_token_vocabulary () - returns a complete set of tokens used for glued-token splitting by combining known tones, known modifiers (loaded from config), and all webcolor names into a single vocabulary set.
│       ├── fuzzy/
│           ├── __init__.py
│           ├── alias_validation.py
│               ├── is_token_fuzzy_match() - This function checks whether a given alias text should be considered a valid fuzzy match to any token in a provided tokens list, using a multi-stage decision process that balances similarity, base-form equivalence, and filtering rules. It normalizes both the alias and tokens to lowercase, then—if the full input_text is provided—first blocks the match if the alias (or its recovered base form) does not appear in the original sentence, or if it appears only as part of a two-word phrase not in the allowed tokens. For each token, it skips comparisons blocked by predefined SEMANTIC_CONFLICTS, calculates a fuzz.ratio similarity score, and accepts immediate matches above min_score unless they fail a rhyming_conflict check. If the score is low, it attempts is_suffix_root_match to catch cases where the words share the same base form despite different endings. For “soft” matches in the 75–min_score range, it applies extra checks: blocks rhyming or semantic conflicts, requires compatible base forms via recover_base, ensures the same starting letter, and then accepts if all pass. Finally, for scores just below the threshold (≥82), it allows matches unless blocked by rhyming. If none of these paths succeed, it rejects the alias as a match.
│               ├── is_valid_singleword_alias() - This function decides if a given single-word alias should be treated as a valid match within the context of an input_text and a set of candidate tokens. It first blocks the alias if it’s already contained inside a longer, multi-word alias that’s been matched earlier, preventing redundant or partial matches from being counted. If it passes that check, it then accepts immediately if the alias exactly matches the entire input_text via is_exact_match. If neither of those conditions applies, it delegates the decision to is_token_fuzzy_match, which performs a detailed fuzzy, base-form, and conflict-aware comparison against the tokens, returning whatever that function decides.
│               ├── is_multiword_alias_match() - This function checks whether a multi-word alias matches an input_text by applying a sequence of fuzzy and token-based matching strategies. It first normalizes both strings (lowercasing, cleaning spaces/punctuation) and tries a fuzz.partial_ratio comparison; if the score meets or exceeds the threshold, it verifies the match by allowing glued-word equivalence (e.g., "rosegold" matching "rose gold") or by ensuring at least two words overlap, rejecting weak partial matches. If that fails, it checks for a two-word alias whose parts match the input’s words in any order, allowing swapped sequences. Finally, it uses fuzz.token_set_ratio to compare word sets, accepting if the score is above the threshold and at least two words are shared. If none of these paths succeed, the alias is rejected as a match.
│               ├── should_accept_multiword_alias() -This function decides if a multi-word alias should be accepted as matching an input_text by progressively testing increasingly lenient similarity criteria. It first normalizes both strings for consistent comparison and accepts immediately if they match exactly. If not, it computes a fuzzy partial_ratio score and accepts if it meets or exceeds the threshold. It then checks for two-word aliases whose parts match the input’s words in any order. If still unmatched, it compares each alias token against all input tokens, counting it as a match if its best fuzzy score is ≥85 and, when strict is enabled, it’s not merely a substring or superstring of any input token; all alias tokens must pass for acceptance. As a last resort, it uses token_set_ratio to compare the overall word sets, accepting if the score is ≥92 and the alias or input is longer than two words.
│               ├── _handle_multiword_alias() - This function determines whether a multi-word alias should be accepted as matching an input_text by first checking for an exact match with is_exact_match, immediately accepting if they are identical. If not, it delegates the decision to is_multiword_alias_match, which applies more flexible fuzzy and token-based matching rules to decide if the alias and input are sufficiently similar.
│               ├── remove_subsumed_matches() - This function filters a list of match strings so that only the most complete, non-redundant matches remain by removing any shorter match that is entirely contained within a longer one. It sorts the matches from longest to shortest, then iterates through them: for each candidate, it checks whether it’s already covered by an existing match in the filtered list—either because the existing match is a single token starting with the candidate, or because the candidate appears as a whole word inside the existing match. If not subsumed, the candidate is kept; otherwise, it’s discarded. The result is a list of matches without duplicates that are just smaller parts of larger matches.
│           ├── conflict_rules.py
│               ├── is_negation_conflict() - This function checks whether two tokens represent a direct negation of each other by normalizing them (lowercasing and cleaning) and then seeing if either one starts with the string "no " followed by exactly the other token. It returns True for exact opposites like "no shimmer" vs "shimmer" in either order, and False otherwise.
│               ├── _is_embedded_alias_conflict() - This function detects when a shorter alias is fully contained within a longer alias but is not identical to it, indicating a potential overlap or collision (e.g., "rose" appearing inside "rosewood"). It returns True in such cases to flag the embedding and False if the strings are identical or the shorter one isn’t contained in the longer.
│           ├── expression_match.py
│               ├── should_accept_alias_match() - This function decides whether an alias should be accepted as matching the input_text by applying a hierarchy of checks. It first normalizes the alias and input text, then accepts immediately if the alias appears as a direct substring in the input. If not, it rejects the alias if it’s already part of an existing, longer alias in matched_aliases to prevent duplicates. Finally, it routes the decision based on whether the alias is multi-word or single-word: multi-word aliases are validated through _handle_multiword_alias, while single-word aliases are checked with is_valid_singleword_alias, both of which apply their respective fuzzy and conflict-aware matching rules.
│               ├── cached_match_expression_aliases() - This function retrieves a validated dictionary of expression definitions from the project’s configuration using load_config("expression_definition", mode="validated_dict") and then passes both the input_text and that dictionary to match_expression_aliases. It returns the resulting set of matched expression aliases, effectively acting as a thin, cached shortcut for running alias matching with the preloaded configuration data.
│               ├── match_expression_aliases() - This function identifies which canonical expressions from an expression_map appear in a given input_text by running a two-pass matching process. In Pass 1, it sorts each expression’s aliases from longest to shortest, then uses should_accept_alias_match to see if any alias matches the input; when one does, it records the expression, marks it as alias-matched, and prevents further alias checks for that expression. It also collects the individual words from matched aliases to skip them later in modifier matching. In Pass 2, it considers expressions not already matched via alias and, if any alias match exists, enforces strict exclusivity by skipping unrelated expressions. For each remaining expression, it checks its modifiers: skipping those already present in alias tokens, accepting exact token matches, or using fuzzy matching (≥90) against either the full input or individual tokens, and scores each expression by how many modifiers matched. It then keeps only those with the highest score to avoid excessive matches from the same input. After both passes, it removes embedded conflicts via _remove_embedded_conflicts, then restores any alias-based matches removed in that step, returning the final set of matched expressions.
│               ├── remove_embedded_conflicts() - This function cleans a set of matched_expressions by removing any whose alias is fully embedded inside a longer alias belonging to another matched expression. For each matched expression, it iterates through its aliases and compares them to the aliases of all other matched expressions; if it finds that an alias is contained within a longer, different alias, it flags that expression for removal. After scanning all pairs, it returns the original set minus any expressions whose aliases were subsumed by longer aliases, effectively preventing partial or overlapping matches from being kept.
│           ├── fuzzy_core.py
│               ├── fuzzy_token_match() - This function compares two tokens and returns a similarity score from 0 to 100, prioritizing exact or recoverable matches over generic fuzzy similarity. It first normalizes both inputs, then returns 100 if they are identical, or if they differ only by a single adjacent letter swap (transposition, e.g., "bleu" ↔ "blue"). Next, it attempts semantic base recovery using _manual_base_recovery to detect if one token is the base form of the other (e.g., "dusty" ↔ "dust"), again returning 100 if successful. If none of these exact or base-recovery matches apply, it computes a fuzzy similarity score via fuzzy_token_score, applies light adjustments (prefix bonus, rhyme penalty), and returns it only if it’s at least 82; otherwise, it returns 0 to indicate the tokens are unrelated.
│               ├── _in_conflict_groups() - This helper function checks whether two normalized tokens belong to the same semantic conflict group while being distinct strings. It supports both dict-based conflict definitions (mapping a key to a set of alternatives) and iterable-based ones (list, tuple, set, frozenset). For dicts, it normalizes the key and its values into a unified set, then returns True if both tokens are in that set but not equal. For iterables, it does the same with each group. If no conflict group contains both tokens, it returns False. This ensures that tokens like "light" and "dark" (if grouped) are not considered matches, even if their fuzzy similarity score is high.
│               ├── is_strong_fuzzy_match() -This function determines whether two tokens should be considered a strong match by normalizing them, then ensuring they’re not in a predefined SEMANTIC_CONFLICTS pair and not a direct negation of each other via is_negation_conflict. If no conflict exists, it computes a similarity score using fuzzy_token_match and returns True only if that score meets or exceeds the given threshold (default 75), indicating a sufficiently close match.
│               ├── is_exact_match() - This function checks whether two strings should be treated as an exact or near-exact match by first normalizing and stripping them down to lowercase alphanumeric characters (removing spaces and punctuation). If the cleaned forms are identical, it returns True. If not, it uses fuzz.ratio to measure their similarity and accepts them as a match if the score is at least 90, allowing for minor spelling differences while still enforcing a high degree of closeness.
│               ├── collapse_duplicates() - This function shortens any sequence of repeated characters in a string s to a single instance of that character by applying a regex substitution. For example, "cooool" becomes "col" and "aaabb" becomes "ab", effectively collapsing consecutive duplicates into one.
│               ├── is_single_transposition() - This function checks whether two strings of equal length differ only by a single adjacent letter swap (transposition). It compares characters at each position, collects the differing positions, and returns True if there are exactly two differences where each character in one string is swapped with the corresponding character in the other (e.g., "form" ↔ "from"), otherwise False.
│               ├── is_single_substitution() - This function checks whether two strings of equal length differ by exactly one character substitution. It compares the strings position by position, counts how many characters differ, and returns True if there is exactly one mismatch (e.g., "blue" ↔ "blus"), otherwise False.
│               ├── fuzzy_match_token_safe() - This function attempts to find the most reliable match for a raw_token within a set of known_tokens using a layered matching strategy that prioritizes exactness and safe transformations before falling back to fuzziness. It first normalizes the token (lowercase, trim) and collapses repeated letters, then checks in order: exact match, single-letter transposition, match after duplicate-letter collapse, match after duplicate collapse plus one substitution, and single-character deletion from a candidate. It also checks if replacing hyphens with spaces produces an exact normalized match. If none of these pass, it computes a fuzzy token_sort_ratio score for all candidates, tracking the best match. Before accepting a fuzzy match, it tries suffix recovery (recover_ish or recover_y) to map derived forms back to known base tokens. If suffix recovery yields a valid known token, it’s returned; otherwise, the best fuzzy match is accepted only if its score meets the threshold. If no match qualifies, it returns None.
│           ├── scoring.py
│               ├── fuzzy_token_score() - This function calculates a fuzzy similarity score between two tokens by averaging fuzz.partial_ratio and fuzz.ratio, then adjusting the result with small bonuses or penalties based on specific patterns. It adds an 8-point bonus if the first three characters of both tokens match, and applies a 12-point penalty for short tokens (≤5 characters) that rhyme but start with different letters (e.g., "ink" vs "pink"). The final score is rounded, clamped between 0 and 100, and returned as a float, giving a weighted measure of how similar the tokens are while discouraging misleading rhyme matches.
│               ├── rhyming_conflict() -  This function detects potential false-positive matches caused by short rhyming words. It returns True if both tokens are 5 characters or fewer, share the same last two letters, but have different first letters (e.g., "ink" vs "pink"), signaling a likely misleading match based solely on rhyme.
│               ├── fuzzy_token_overlap_count() - This function counts the number of tokens in a_tokens that match any token in b_tokens, considering both exact matches and fuzzy matches with a similarity score of at least 85. It iterates through each token in the first list, compares it to all tokens in the second list, and increments the count when a match is found—accepting soft variants like "soft" ↔ "softy" or "pink" ↔ "pinky". Each a token is counted at most once, even if multiple b tokens qualify.
│       ├── token/
│           ├── __init__.py
│           ├── base_recovery.py
│               ├── recover_base()  - Normalizes a token (lowercase, strip spaces/hyphens/underscores), early-returns None if empty, and recovers its canonical base via a fixed sequence: check RECOVER_BASE_OVERRIDES; run suffix recovery functions with one-level recursive chaining; accept direct hits in the known modifiers or tones; then, at depth 0 and if allow_fuzzy is True, attempt a fuzzy match over the union of known tokens (allowed for len(raw) ≥ 4, or len(raw) == 3 alphabetic with a stricter threshold and a first-letter anchor), rejecting candidates blocked by BLOCKED_TOKENS or SEMANTIC_CONFLICTS; finally, if fuzzy is absent/blocked, apply a short-token abbreviation fallback for 3–4 letter alphabetic inputs using a consonant-skeleton match (optionally treating “y” as a vowel via VOWELS_CONS/VOWELS_VOW), anchored on first letter and preferring modifiers over tones and shorter candidates. Returns the resolved base or None. Legacy kwargs (fuzzy_fallback→allow_fuzzy, known_modifiers, known_tones, use_cache, fuzzy_threshold) remain supported.
│               ├── recover_base_cached_with_params()  - LRU-cached wrapper around recover_base that stores up to 10,000 results and returns cached bases for repeated tokens without re-running suffix or fuzzy logic; uses frozenset keys for stability, canonical known modifier and tone sets by default, and honors allow_fuzzy/fuzzy_threshold when passed.
│               ├── _recover_base_impl()  - Internal helper that executes the pipeline for pre-normalized raw: overrides → ordered SUFFIX_RECOVERY_FUNCS (with one-level chained recovery) → direct membership check → top-level fuzzy (with revised gates for short tokens, BLOCKED_TOKENS/SEMANTIC_CONFLICTS guards, and stricter threshold for 3-char inputs); if fuzzy produces a guarded candidate (e.g., fails the first-letter check), it falls through instead of returning; then runs the 3–4 letter abbreviation (consonant-skeleton) fallback, using module-level vowel sets (VOWELS_CONS, VOWELS_VOW), anchored on the first letter and preferring modifiers/shorter candidates. Returns the resolved base or None. Debug tracing is handled through logger.debug when debug=True.
│               ├── _is_known_token()  - Checks whether a given token is recognized as a valid modifier or tone by testing membership in the canonical sets. Fast filter for tokens meaningful within the controlled vocabularies.
│               ├── _is_semantic_conflict()  - Determines if two tokens represent a semantic conflict by checking against SEMANTIC_CONFLICTS (dict or iterable of groups). Both tokens are lowercased; if they belong to the same conflict group but are not identical, returns True, signaling a clash. Prevents incompatible resolutions (e.g., “airy” vs. “fairy”).
│               ├── _match_override()  - Attempts to map a normalized token to a canonical base form using RECOVER_BASE_OVERRIDES. If overrides are a dict, returns the mapped base only if valid; if an iterable of (src,dst) or sets, applies those rules and, in the case of sets, selects the shortest valid candidate deterministically. Returns None if no rule matches. Provides controlled exception handling beyond suffix/fuzzy recovery.
│               ├── is_known_modifier()  - Returns True if token is in the global known_modifiers set.
│               ├── is_known_tone()  - Returns True if token is in the global known_tones set.
│           ├── normalize.py - Shared utilities for token normalization and analysis.
│               ├── singularize(text: str) - Back-compatible entrypoint that singularizes a single word or only the last word of a phrase using safe rules (lowercase/trim; protect invariants like “series”/“species”; apply ies→y; remove “es” when ending with sh/ch/x/z/ss/oes; else drop a final “s” for words >3 chars), leaving all preceding tokens untouched and gracefully handling empty/non-string inputs.
│               ├── normalize_token(token: str, keep_hyphens: bool = False) - Produces a canonical token/phrase by lowercasing, trimming, converting underscores to spaces, collapsing whitespace, and either tightening hyphens (if keep_hyphens=True) or replacing them with spaces; finally singularizes only the last token when it is a cosmetic noun (driven by COSMETIC_NOUNS) to stabilize downstream recovery, fuzzy matching, and extraction.
│               ├── get_tokens_and_counts(text: str, keep_hyphens: bool = False) - Regex-tokenizes with `[a-z]+(?:-[a-z]+)?`, normalizes each token using the same hyphen policy, splits any multi-word results produced by normalization, and aggregates frequencies into a `{token: count}` dictionary for consistent analytics.
│               ├── _singularize_word(w: str) - Internal helper that implements the singular rules used by public APIs: validate/trim, short-circuit invariants (INVARIANT_SINGULARS), transform ies→y, remove “es” when ending with sh/ch/x/z/ss/oes, otherwise remove a trailing “s” for words longer than three characters; return the original token when no rule applies.
│               ├── _singularize_phrase_if_cosmetic_last(text: str) - Internal gate that inspects only the last token of a phrase and applies `_singularize_word` iff that token (after cleanup) is present in COSMETIC_NOUNS or its singular form is, ensuring domain-scoped singularization without touching modifiers or color adjectives.
│               └── INVARIANT_SINGULARS - Control sets for deterministic behavior: INVARIANT_SINGULARS lists nouns that never change at singular (e.g., “series”, “species”);
│           ├── split/
│               ├── __init__.py
│               ├── split_core.py
│                   ├── has_token_overlap() - This function checks whether two strings share at least one word in common. It splits both strings into tokens, converts them to sets, and returns True if the intersection is non-empty, meaning at least one token appears in both, otherwise False.
│                   ├── fallback_split_on_longest_substring () - This function tries to split a token by finding the longest substring within it that exists in a given vocabulary. It sorts the vocabulary from longest to shortest, then scans for the first occurrence of any known substring inside the token. If found, it returns a list composed of the token’s prefix (if any), the matched substring, and the suffix (if any). For example, with token "dustyroseglow" and vocab containing "rose", it would return ["dusty", "rose", "glow"]. If no substring from the vocab is found, it returns an empty list.
│                   ├── recursive_token_split() - This function attempts to break down a possibly glued token into valid smaller parts by using a recursive splitting strategy guided by an is_valid callback. Inside, it defines a cached helper _split that first accepts a token as-is if valid, otherwise tries splitting it between positions 3 and 10, recursively checking if both halves can be resolved into valid pieces; if so, it returns the concatenation of the splits. If no full decomposition works, it falls back to scanning for a valid prefix (longest-first) or a valid suffix (shortest-first), returning whichever is found. If neither recursive splits nor fallbacks succeed, it returns None. This allows recovery of glued forms like "dustyrose" into ["dusty","rose"].
│           ├── suffix/
│           ├── __init__.py
│               ├── recovery.py
│                   ├── build_augmented_suffix_vocab() -  This function builds an expanded vocabulary of cosmetic-related tokens by taking the union of known tones, modifiers, and webcolor names, then generating plausible suffix variants for each. For every input token, it first attempts to recover its base form using recover_base (with overrides or manual fallbacks for tricky endings like -y or -ey). If a valid base is found and not blocked by NON_SUFFIXABLE_MODIFIERS, it creates a set of forms including the raw token, the base, and derived variants: -y (via build_y_variant or forced inclusion if present in known modifiers), -ey (only if explicitly allowlisted), and multiple -ed forms depending on rules — standard +ed, consonant-vowel-consonant doubling, replacing y or e endings, or allowlisted raw variants. It also generates doubled CVC+y forms when appropriate. Each batch of forms is added to the augmented set, which is returned as the full suffix-augmented vocabulary, with optional debug output showing step-by-step expansions.
│                   ├── is_suffix_variant() - This function determines whether a token is a valid suffix-derived variant (ending in -y or -ed) of a known modifier or tone. It first rejects tokens that don’t end in those suffixes, or that are already in the known sets (unless explicitly overridden). If the form looks like a suffix, it calls recover_base to try to resolve the underlying base word, optionally allowing fuzzy recovery. The result is considered valid only if the recovered base is in the known modifiers or tones and is not listed in NON_SUFFIXABLE_MODIFIERS. With debug enabled, it logs each decision, and the function ultimately returns True for valid suffix variants and False otherwise.
│                   ├── recover_y - This function attempts to recover the base form of a token ending in “-y” by stripping or adjusting the suffix and testing possible candidates against known modifiers and tones. It first normalizes the token and skips it if it’s too short or doesn’t end in “y”. If the token has a direct mapping in RECOVER_BASE_OVERRIDES, that override is returned immediately. Otherwise, it generates candidate bases by removing the final “y” (e.g., dusty → dust), collapsing doubled consonants (e.g., fuzzy → fuz), appending “e” after stripping (e.g., creamy → creame), or stripping an extra “y” if present (e.g., glossyy → gloss). Each candidate is checked against known modifiers and tones, and the first valid match is returned; if none match, the function returns None.
│                   ├── recover_ed() - This function tries to recover the base form of a token ending in “-ed” by testing plausible base candidates against known modifiers and tones. It first skips tokens that don’t end with “ed” or are too short. Otherwise, it strips the “ed” and generates candidates: appending back an “e” (faded → fade), collapsing doubled consonants for consonant-vowel-consonant endings (tapped → tap), and finally using the raw stripped base itself (muted → mut). Each candidate is checked against the known sets, and the first valid match is returned; if none match, it returns None.
│                   ├── recover_ing() - This function attempts to recover the base form of a token ending in “-ing” by stripping the suffix and testing possible candidates. It first skips tokens that don’t end with “ing” or are too short. Otherwise, it removes the “ing” to form a base and generates candidates: the plain base (glowing → glow) and the base plus “e” (glowing → glowe). It then checks each candidate against the known modifiers and tones, returning the first match found; if none are valid, it returns None.
│                   ├── recover_ied() - This function recovers the base form of a token ending in “-ied” by replacing the suffix with “y”. It first skips tokens that are too short or don’t end with “ied”. Otherwise, it constructs a candidate by dropping “ied” and appending “y” (e.g., tried → try). If this candidate exists in the known modifiers or tones, it is returned; if not, the function returns None.
│                   ├── recover_er() - This function attempts to recover the base form of a token ending in “-er”, typically from comparative adjectives. It skips tokens that don’t end with “er” or are too short, then strips the suffix to form a candidate (e.g., darker → dark). If that candidate exists in the known modifiers or tones, it is returned; otherwise, the function returns None.
│                   ├── recover_ier() - This function recovers the base form of a token ending in “-ier”, typically comparatives of adjectives. It first skips tokens that don’t meet the suffix or length requirements, then strips off “ier” to get a stem (e.g., trendier → trend). If that stem is already in the known modifiers or tones, it returns it. Otherwise, it tries forming a “-y” variant (trendier → trendy) and returns it if valid. If that y-form itself has an override defined in RECOVER_BASE_OVERRIDES, the override is checked against known sets and returned if valid (e.g., fancier → fancy → override). If none of these candidates work, the function returns None.
│                   ├── recover_ish() - The recover_ness() function is designed to normalize tokens ending in “-ness” by stripping that suffix and testing whether the remaining stem is a valid known modifier or tone. It typically rejects tokens that don’t end with "ness" or are too short, then removes the suffix (e.g., softness → soft), and if the resulting base exists in the known sets, it returns that base; otherwise, it returns None. This makes it a targeted recovery rule for abstract noun forms that should map back to their descriptive adjective bases.
│                   ├── recover_ness() - This function recovers the base form of a token ending in “-ness” by stripping the suffix and testing plausible stems against known modifiers and tones. It first skips tokens that don’t end with "ness" or are too short, then removes the suffix to form a base (e.g., softness → soft). If the base ends in "i", it tries collapsing it to map happiness → happy; if that collapsed form exists in the known sets, it returns it. Otherwise, it checks the stripped base directly and returns it if valid. If neither candidate is recognized, the function returns None.
│                   ├── recover_ly() - This function tries to recover the base form of tokens ending in “-ly”, typically adverbs derived from adjectives. It first skips tokens that are too short or don’t end with "ly". For special cases ending in "ally", it replaces that suffix with "ic" (e.g., emotionally → emotic). Otherwise, it strips just the "ly" to form the candidate (e.g., softly → soft). It then checks whether the candidate exists in the known modifiers or tones and, if so, returns it; if not, it returns None.
│                   ├── recover_en() - This function attempts to recover the base form of a token ending in “-en”, often used for adjectives derived from nouns (e.g., golden). It first skips tokens that don’t end in "en" or are too short, then strips the suffix to form the candidate (e.g., golden → gold). If the candidate exists in the known modifiers or tones, it is returned; otherwise, the function returns None.
│                   ├── recover_ey() - This function recovers the base form of tokens ending in “-ey”, which are often informal variants of adjectives (e.g., bronzey, beigey). It first skips tokens that don’t meet the suffix or length requirement, then strips "ey" to form a base and constructs a candidate by appending "e" (bronzey → bronze). If that candidate exists in the known modifiers or tones, it returns it. If not, it checks whether the stripped base is included in a special allowlist (Y_SUFFIX_ALLOWLIST) and returns it if allowed (beigey → beige). If neither condition succeeds, the function returns None.
│                   ├── _collapse_double_consonant() - This function fixes cases where a base token ends with an unnecessary double consonant. If the last two characters are the same (e.g., redd), it collapses them into a single consonant (red) and checks whether the result exists in the known modifiers or tones. If valid, it returns the collapsed form; if not, or if the base doesn’t end with a double letter, it returns None.
│                   ├── recover_ee_to_y() - This function recovers the base form of tokens ending in “-ee” by transforming them into a “-y” form. It first skips tokens that don’t end with "ee" or are too short, then replaces the suffix with "y" (e.g., ivoree → ivory). If the resulting candidate exists in the known modifiers or tones, it returns that base; otherwise, it returns None.
│               ├── registry.py
│                   ├── SUFFIX_RECOVERY_FUNCS - This list defines the ordered set of functions used for suffix-based token recovery. Each function applies a specific heuristic to strip or transform common suffixes (like -y, -ed, -ee → y, -ing, -ish, -ly, -en, -ness, -ier, -er, -ied) and map a derived or inflected token back to its canonical base form if it exists in the known modifiers or tones. When recover_base runs, it iterates through this list in sequence, trying each recovery strategy until one succeeds.
│     ├── utils/
│           ├── __init__.py
│         ├── load_config.py
│               ├── _default_data_dir() - Walks upward from a given start path (or this file) through its parents to locate the first directory named "data" (preferred) or "Data", returns the absolute resolved Path if found, otherwise raises DataDirNotFound to signal that no configuration directory exists.
│               ├── load_config() - Loads a JSON configuration file from the resolved data directory (base_dir overrideable or via DATA_DIR environment variable), builds the full path with ".json" extension, parses the file with UTF-8, and depending on mode returns raw JSON (any type), a frozenset of strings from a list, or a validated dict (optionally transformed by a user-supplied validator); caches results by (path, mode, encoding) unless a validator is provided, and raises ConfigFileNotFound if file missing/unreadable, ConfigParseError if JSON invalid or validator fails, and ConfigTypeError if the content type does not match the requested mode.
│               ├── clear_config_cache() - Clears the in-memory configuration cache (keyed by path, mode, encoding, validator flag) so that subsequent calls reload JSON from disk, mainly used in test suites or to force hot reload in long-running processes.
│         ├── nlp_utils.py - Natural language helpers for token-level semantic analysis.
│           ├── _normalized_antonyms() - Internal helper that collects and normalizes all antonyms of a given word from WordNet using the project normalizer; returns an empty set if the word is empty or if WordNet resources are missing.
│           ├── are_antonyms() - Public function that checks if two words are antonyms by normalizing both and testing membership in each other’s normalized antonym set; uses caching for efficiency and safely returns False if inputs are empty or WordNet data is unavailable.
│           ├── _get_spacy() - Initializes and caches a global spaCy language model once; attempts to load "en_core_web_sm" with ner/parser/textcat disabled, and if loading fails or spaCy is missing it disables lemmatization (returns None instead of falling back to a blank model).
│           ├── lemmatize_token() - Lemmatizes a single token using spaCy when available, otherwise returns the normalized token unchanged; validates that the input is a non-empty string, applies project normalization, and returns the lemma of the first token (or text itself if unavailable), with results cached for up to 20,000 calls.
│         ├── log.py
│           ├── debug() - Prints a timestamped debug line including topic and level to the chosen stream if CHATBOT_DEBUG_TOPICS contains the topic name (or "all"), with support for dynamic reload via reload_topics().
│     ├── sentiment_router.py
│       ├── build_color_sentiment_sumamry() - This function builds a summary of color information tied to a specific sentiment label (e.g., “romantic”, “edgy”) by aggregating color phrases and their RGB values across multiple user segments. It first calls aggregate_color_phrase_results to extract tones, matched color phrases, and any new local phrase→RGB mappings from the given segments. These local mappings are merged into the global rgb_map cache, while format_tone_modifier_mappings is invoked to optionally log or structure the results for downstream use. It then picks a representative RGB value from the local matches using choose_representative_rgb, stores it in base_rgb_by_sentiment under the current sentiment, and finally returns a dictionary containing the sorted list of matched color phrases, the chosen base RGB (or None if none matched), and the global distance threshold constant used for RGB comparisons.
│     ├── sentiment_core.py
│       ├── _map_bart_label_to_sentiment() - This function maps a text classification label produced by a BART model back to a normalized sentiment category. It first tries to find the label’s exact index in _CANDIDATE_LABELS and returns the corresponding entry from _CANDIDATE_SENTIMENTS. If the label isn’t found, it retries with a case-insensitive comparison. If no match is found at all, it defaults to returning "neutral". This avoids using a separate mapping dictionary while ensuring robustness to capitalization differences.
│       ├── get_nlp() - This function provides access to a cached spaCy language model. It checks whether the global variable _nlp has already been initialized; if not, it loads the default English model "en_core_web_sm" and stores it in _nlp. On subsequent calls, it simply returns the cached model, avoiding repeated loading overhead.
│       ├── get_vader() - This function returns a cached instance of NLTK’s VADER sentiment analyzer. If the global _vader has not been initialized, it first checks whether the vader_lexicon resource is available; if not, it downloads it. It then creates a SentimentIntensityAnalyzer object, stores it in _vader, and returns it. On subsequent calls, it simply returns the cached analyzer, avoiding repeated setup.
│       ├── get_sentiment_pipeline() - This function returns a cached Hugging Face transformers pipeline for zero-shot sentiment classification. If the global _sentiment_pipeline hasn’t been initialized yet, it creates one using the pre-defined model name _SENTIMENT_MODEL_NAME (e.g., "facebook/bart-large-mnli"), stores it in _sentiment_pipeline, and returns it. On later calls, it simply returns the cached pipeline to avoid reloading the model.
│       ├── ensure_negex() - This function ensures that the spaCy pipeline has the negspaCy negation detector attached and returns the active NLP object. It checks a global _negex_ready flag to avoid repeated setup: if False, it returns the plain spaCy model; if True and the "negex" component is already present, it returns it directly. Otherwise, it tries to import and add the Negex pipe with default clinical English rules. If successful, it marks _negex_ready = True; if not (e.g., negspaCy isn’t installed), it sets _negex_ready = False. In all cases, it returns the spaCy pipeline, with or without negation support.
│       ├── _vader_score() - This function computes a sentiment score for a given text using NLTK’s VADER analyzer. It calls get_vader() to obtain the sentiment analyzer, runs polarity_scores(text), and returns the "compound" score (a float between –1 and 1). If any error occurs (e.g., VADER not available), it logs the exception and safely returns 0.0 as a fallback.
│       ├── classify_segments_by_sentiment_no_neutral() - This function classifies a list of text segments into positive or negative sentiment categories, deliberately excluding “neutral.” For each segment, it first applies detect_sentiment (a hybrid of VADER and BART), then refines the result with map_sentiment, which accounts for negation and re-checks with VADER. If the mapped result is neither positive nor negative (i.e., neutral), it forces the classification to positive as a design choice. All segments are collected into a dictionary with keys "positive" and "negative", and in case of any errors during processing, the exception is logged but the loop continues.
│       ├── detect_sentiment() -This function determines whether a given text expresses positive, negative, or neutral sentiment using a hybrid two-stage approach. It first applies VADER sentiment analysis: if the compound score is above a positive threshold, it returns "positive"; if below a negative threshold, it returns "negative". If the score is near-neutral, it falls back to a BART zero-shot classifier, running the text against predefined candidate labels and mapping the top-scoring label back to a sentiment category. For testing, mock vader or bart analyzers can be injected instead of the real models. If anything fails during processing, the function logs the error and defaults to returning "neutral".
│       ├── map_sentiment() - This function refines a predicted sentiment label by applying negation overrides and fallback checks. It first calls is_negated_or_soft to detect hard or soft negation in the text, and if found, immediately forces the sentiment to "negative". If the input predicted sentiment is already "positive" or "negative", it returns it unchanged. If the sentiment is "neutral", it re-checks the text with VADER: if the score is above the positive threshold, it returns "positive"; if below the negative threshold, it returns "negative"; otherwise, it keeps "neutral". This ensures that negation always takes precedence, and ambiguous neutrals get a second chance at resolution.
│       ├── contains_sentiment_splitter_with_segment() - This function analyzes a sentence to decide whether it contains a clause-level sentiment split and, if so, returns the split segments. It first processes the text with spaCy. If the structure matches a skip pattern (negation + “or” without punctuation), it avoids splitting and returns the whole text as one segment. Otherwise, it tries to locate a clause boundary with find_splitter_index. If the candidate splitter is at the beginning and comes from an adverb (advmod), it only accepts it if the word is listed in a configurable set of discourse adverbs; otherwise, the split is ignored. If a valid index is found, the text is split at that point via split_text_on_index, and only considered a true split if at least two non-empty segments result. If no dependency-based splitter is valid, it falls back to punctuation-based segmentation. The return value is a tuple (has_split, segments).
│       ├── should_skip_split_due_to_or_negation() - This function decides whether a sentence should skip clause-level splitting when analyzing sentiment. It checks the spaCy doc for three conditions: the presence of a negation token (dep_ == "neg"), the word "or", and any punctuation mark among . , ;. It returns True only if the sentence contains both a negation and “or” but has no punctuation, signaling that a dependency-based split would likely misrepresent the sentiment. Otherwise, it returns False.
│       ├── find_splitter_index() - This function scans a spaCy-parsed sentence (doc) to locate the index of a clause-level splitter token, which indicates where sentiment might shift. It iterates over tokens and prioritizes coordinating conjunctions (dep_ == "cc") like and or or, but skips them if is_tone_conjunction determines they’re joining color tones rather than clauses. It also considers subordinating markers (mark) and discourse markers as valid split points, and as a fallback accepts a sentence-initial adverb (advmod at position 0). If no suitable splitter is found, it returns None.
│       ├── is_tone_conjunction() - This function determines whether a conjunction in a sentence should be treated as joining tone-like words (e.g., matte or glossy) rather than splitting the sentence into separate clauses. Given a spaCy doc and a conjunction index, it inspects the neighboring tokens: if both are adjectives, noun-like words, participles, past-tense verbs used as adjectives, or words with descriptive suffixes (-y, -ish), it considers them tone-like. It allows standard conjunction markers (and, or, /, &) but rejects plain verbs unless they look adjectival. An optional antonym_fn can disqualify adjective pairs if they are antonyms (e.g., light or dark should split). It also has override rules to accept certain "X or Y" or "X and Y" patterns even if the initial checks fail, such as longer noun/adjective forms or paired -y adjectives. With debug enabled, it prints detailed reasoning, and finally returns True if the conjunction should be considered a tone conjunction (thus no clause split), or False otherwise.
│       ├── split_text_on_index() - This function splits a spaCy doc into text segments around a given token index i, adapting its behavior depending on the token’s position. If i is the first token, it discards it and returns segments from the remainder, further splitting on semicolons or commas. If i is the last token, it returns segments from everything before it, again splitting on semicolons or commas. If i is in the middle, it creates exactly two segments: the text before the token and the text after it. All returned parts are stripped of extra spaces, and empty segments are removed.
│       ├── fallback_split_on_punctuation() - This function provides a last-resort sentence splitter by breaking text on punctuation marks (. ; ,). It trims each resulting piece and discards empties. If the split yields two or more non-empty segments, it returns (True, segments) to indicate a successful split; otherwise, it returns (False, [original text]), signaling that no meaningful segmentation occurred.
│       ├── _split_sentence_with_separators() - This function heuristically splits a sentence into clauses while preserving separators. It first attempts a dependency-based split at a detected conjunction/marker, returning both sides with the split token as separator. If suppressed splits occur (e.g., “Choose matte or choose glossy”), it rescues them when neighbors involve verbs or auxiliaries. Failing that, it falls back to punctuation-based splitting, capturing punctuation marks as separators alongside the resulting clauses. If no split is possible, it returns the entire text as a single clause with a None separator.
│       ├── analyze_sentence_with_separators() -This function provides a high-level sentiment analysis of full sentences. It first splits the sentence into clauses with _split_sentence_with_separators(). Each clause is then analyzed: sentiment is predicted with detect_sentiment(), refined with map_sentiment(), and paired with its separator. It returns a list of dicts containing {"clause", "polarity", "separator"}. If an error occurs, it falls back to returning the whole sentence as one neutral clause.
│       ├── is_negated_or_soft() - This function analyzes a sentence to decide if it contains hard or soft negation: it parses the text with spaCy, checks for a soft negation motif like “not too ADJ” (e.g. “not too bad”), and otherwise looks for hard cues such as dependency neg, morphological Polarity=Neg, “no” as a determiner (“no problem”), or pronouns starting with “no” (“nobody”). If a hard cue is found, it returns (True, False); if only a soft motif exists, (False, True); otherwise (False, False), with optional debug output showing which rule fired.
│       ├── def is_negated() - This function is a simple wrapper around is_negated_or_soft, returning only the hard negation flag (the first element of the tuple), so it yields True if the text contains a hard negation cue (like “not good”, “never liked”), and False otherwise.
│       ├── is_softly_negated() - This function is a thin wrapper around is_negated_or_soft, returning only the soft negation flag (the second element of the tuple), so it yields True if the text matches a soft negation pattern (like “not too sure” or “no too easy”), and False otherwise.



